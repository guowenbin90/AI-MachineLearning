# Introduction to Machine Learning
[Scikit Learn](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
## Navie Bayes
- Gaussian Naive Bayes
- Multinomial Naive Bayes
- Complement Naive Bayes
- Bernoulli Naive Bayes
- Categorical Naive Bayes
## Support Vector Machines
- Classification
- Regression
## Decision Trees
- Classification
- Regression
## Choose Algorithm
- k nearest neighbors: classic, simple, easy to understnd
- adaboot & random forest: 'emsemble methods', meta classifiers built from decision trees
### Process
1. do some research
2. find sklearn documenttion
3. deploy it
4. use it to make predictions
5. evaluate it accuracy
## Datasets and Questions
- Patterns
### Types of Data
- numerical - numerical values (numbers, e.g. salary)  
- categorical - limited number of discrete values (category, e.g. star of movie)
- time series - temporal value (date, timestamp)
- text - words
## Regressions
- Continous supervised learning
- Minimize sum of the squared errors (SSE)
1. Ordinary Least Squares
2. Stochastic Gradient Descent  
- r<sup>2</sup> (r squared)(0<r<sup>2</sup><1): how much of my change in the output (y) is explained by the change in my input  

Comparing Classification & Regression
| Property   |      Supervised classification      |  Regression |
|:----------|:-------------|:------|
| Output type |  Discrete (class labels) | Continous (number) |
| What are you tring to find | Decision boundary | "best fit line" |
| Evaluation | Accuracy | "Sum of squared error" or r<sup>2</sup>("r squared") |

## Outliers

## Clustering
- K-means
## Feature Scaling
